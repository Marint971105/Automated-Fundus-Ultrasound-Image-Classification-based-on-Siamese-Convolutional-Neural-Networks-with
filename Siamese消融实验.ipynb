{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import json\n",
    "\n",
    "from skimage.color import rgb2gray\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import copy\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.transforms import ToTensor,Resize\n",
    "\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import f1_score,accuracy_score,recall_score,classification_report,confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(foldername = 'dataset/', train_size = 310, test_size = 110):\n",
    "\n",
    "    fileList = []\n",
    "    for dirname, _, filenames in os.walk(foldername):\n",
    "#         print(dirname)\n",
    "#         print(filenames)\n",
    "        for filename in filenames:\n",
    "            if filename =='666.jpg' or filename == '11702.jpg':\n",
    "                continue\n",
    "            fileList.append(os.path.join(dirname, filename))\n",
    "\n",
    "    \n",
    "    classList = []\n",
    "\n",
    "    for i in range(len(fileList)):\n",
    "        if 'hunzhuo' in fileList[i]:\n",
    "            classList.append('hunzhuo')\n",
    "        else:\n",
    "            classList.append('tuoli')\n",
    "\n",
    "    df = pd.DataFrame({'filepath':fileList,'label':classList})  \n",
    "\n",
    "    trainDf,testDf, trainy,testy = train_test_split(df, df.label, train_size = train_size, test_size =test_size)\n",
    "\n",
    "    return trainDf, testDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf, testDf = load_data(train_size = 310, test_size = 110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           filepath    label\n",
      "361      dataset/tuoli/tuoli140.jpg    tuoli\n",
      "289       dataset/tuoli/tuoli68.jpg    tuoli\n",
      "198  dataset/hunzhuo/hunzhuo198.jpg  hunzhuo\n",
      "170  dataset/hunzhuo/hunzhuo170.jpg  hunzhuo\n",
      "249       dataset/tuoli/tuoli28.jpg    tuoli\n",
      "260       dataset/tuoli/tuoli39.jpg    tuoli\n",
      "381      dataset/tuoli/tuoli160.jpg    tuoli\n",
      "95    dataset/hunzhuo/hunzhuo95.jpg  hunzhuo\n",
      "65    dataset/hunzhuo/hunzhuo65.jpg  hunzhuo\n",
      "106  dataset/hunzhuo/hunzhuo106.jpg  hunzhuo\n",
      "323      dataset/tuoli/tuoli102.jpg    tuoli\n",
      "205  dataset/hunzhuo/hunzhuo205.jpg  hunzhuo\n",
      "33    dataset/hunzhuo/hunzhuo33.jpg  hunzhuo\n",
      "158  dataset/hunzhuo/hunzhuo158.jpg  hunzhuo\n",
      "322      dataset/tuoli/tuoli101.jpg    tuoli\n",
      "114  dataset/hunzhuo/hunzhuo114.jpg  hunzhuo\n",
      "282       dataset/tuoli/tuoli61.jpg    tuoli\n",
      "71    dataset/hunzhuo/hunzhuo71.jpg  hunzhuo\n",
      "104  dataset/hunzhuo/hunzhuo104.jpg  hunzhuo\n",
      "138  dataset/hunzhuo/hunzhuo138.jpg  hunzhuo\n",
      "265       dataset/tuoli/tuoli44.jpg    tuoli\n",
      "117  dataset/hunzhuo/hunzhuo117.jpg  hunzhuo\n",
      "32    dataset/hunzhuo/hunzhuo32.jpg  hunzhuo\n",
      "107  dataset/hunzhuo/hunzhuo107.jpg  hunzhuo\n",
      "391      dataset/tuoli/tuoli170.jpg    tuoli\n",
      "13    dataset/hunzhuo/hunzhuo13.jpg  hunzhuo\n",
      "254       dataset/tuoli/tuoli33.jpg    tuoli\n",
      "403      dataset/tuoli/tuoli182.jpg    tuoli\n",
      "345      dataset/tuoli/tuoli124.jpg    tuoli\n",
      "347      dataset/tuoli/tuoli126.jpg    tuoli\n",
      "..                              ...      ...\n",
      "199  dataset/hunzhuo/hunzhuo199.jpg  hunzhuo\n",
      "339      dataset/tuoli/tuoli118.jpg    tuoli\n",
      "1      dataset/hunzhuo/hunzhuo1.jpg  hunzhuo\n",
      "229        dataset/tuoli/tuoli8.jpg    tuoli\n",
      "73    dataset/hunzhuo/hunzhuo73.jpg  hunzhuo\n",
      "203  dataset/hunzhuo/hunzhuo203.jpg  hunzhuo\n",
      "431      dataset/tuoli/tuoli210.jpg    tuoli\n",
      "336      dataset/tuoli/tuoli115.jpg    tuoli\n",
      "304       dataset/tuoli/tuoli83.jpg    tuoli\n",
      "112  dataset/hunzhuo/hunzhuo112.jpg  hunzhuo\n",
      "88    dataset/hunzhuo/hunzhuo88.jpg  hunzhuo\n",
      "237       dataset/tuoli/tuoli16.jpg    tuoli\n",
      "264       dataset/tuoli/tuoli43.jpg    tuoli\n",
      "82    dataset/hunzhuo/hunzhuo82.jpg  hunzhuo\n",
      "127  dataset/hunzhuo/hunzhuo127.jpg  hunzhuo\n",
      "150  dataset/hunzhuo/hunzhuo150.jpg  hunzhuo\n",
      "50    dataset/hunzhuo/hunzhuo50.jpg  hunzhuo\n",
      "8      dataset/hunzhuo/hunzhuo8.jpg  hunzhuo\n",
      "411      dataset/tuoli/tuoli190.jpg    tuoli\n",
      "257       dataset/tuoli/tuoli36.jpg    tuoli\n",
      "236       dataset/tuoli/tuoli15.jpg    tuoli\n",
      "139  dataset/hunzhuo/hunzhuo139.jpg  hunzhuo\n",
      "270       dataset/tuoli/tuoli49.jpg    tuoli\n",
      "338      dataset/tuoli/tuoli117.jpg    tuoli\n",
      "399      dataset/tuoli/tuoli178.jpg    tuoli\n",
      "365      dataset/tuoli/tuoli144.jpg    tuoli\n",
      "153  dataset/hunzhuo/hunzhuo153.jpg  hunzhuo\n",
      "416      dataset/tuoli/tuoli195.jpg    tuoli\n",
      "424      dataset/tuoli/tuoli203.jpg    tuoli\n",
      "263       dataset/tuoli/tuoli42.jpg    tuoli\n",
      "\n",
      "[310 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(trainDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeat(net, imgList):\n",
    "\n",
    "    net.eval()\n",
    "    featList = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(imgList)):\n",
    "            feat = net.getFeatures(imgList[i])\n",
    "            featList.append(feat.squeeze(0).detach().cpu().numpy())\n",
    "\n",
    "    return np.array(featList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCDDistribution(net,imgList, labelList ):\n",
    "\n",
    "    uniqueClasses = [0,1]\n",
    "\n",
    "    NUM_COLORS = 2\n",
    "\n",
    "    # featList = []\n",
    "\n",
    "    # for i in range(len(imgList)):\n",
    "    #     feat = net.getFeatures(imgList[i])\n",
    "    #     featList.append(feat.squeeze(0).detach().cpu().numpy())\n",
    "\n",
    "\n",
    "    get_color = plt.get_cmap('viridis')\n",
    "\n",
    "    colorMap = {}\n",
    "    for i in range(len(uniqueClasses)):\n",
    "        colorMap[uniqueClasses[i]] = get_color(i/NUM_COLORS)\n",
    "    \n",
    "    support = getFeat(net, imgList)\n",
    "    X = support\n",
    "\n",
    "    print(X.shape)\n",
    "\n",
    "    # scaler = StandardScaler()\n",
    "    # X = scaler.fit_transform(support)\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    pcaX = pca.fit_transform(X=X)\n",
    "    varExplained = np.sum(pca.explained_variance_ratio_[:2])\n",
    "    print(f\"Variance Explained : {varExplained}\")\n",
    "\n",
    "    colors = [colorMap[int(x)] for x in labelList]\n",
    "    for i in range(len(uniqueClasses)):\n",
    "      index = np.array(labelList) == int(uniqueClasses[i])\n",
    "      plt.scatter(pcaX[index,0], pcaX[index,1], color=colorMap[int(uniqueClasses[i])], label =int(uniqueClasses[i]))\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSupport(net, dataloader):\n",
    "\n",
    "    supports = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        net.eval()\n",
    "\n",
    "        for data in dataloader:\n",
    "            x,y = data\n",
    "            x = x.to('cuda')\n",
    "            feat = net.getFeatures(x)\n",
    "\n",
    "            supports.append(feat)\n",
    "            labels.append(y)\n",
    "\n",
    "    support = torch.cat(supports,dim=0)\n",
    "    labels = torch.cat(labels,dim=0)\n",
    "\n",
    "    return support, labels\n",
    "\n",
    "\n",
    "def compareSamples(index1,index2, metric = 'cosine'):\n",
    "    img1 = testX[index1].unsqueeze(0).repeat(3,1,1)\n",
    "    img2 = testX[index2].unsqueeze(0).repeat(3,1,1)\n",
    "\n",
    "    label1, label2 = testy[index1], testy[index2]\n",
    "\n",
    "    if label1 ==label2:\n",
    "        trainLabel = 1\n",
    "    else:\n",
    "        trainLabel = 0\n",
    "\n",
    "    print(f\"Labels are {label1} and {label2}\")\n",
    "\n",
    "\n",
    "    feat1 = net.getFeatures(img1.unsqueeze(0).cuda().float())\n",
    "    feat2 = net.getFeatures(img2.unsqueeze(0).cuda().float())\n",
    "\n",
    "    if metric=='cosine':\n",
    "      dist = F.cosine_similarity(feat1, feat2)\n",
    "    else:\n",
    "      dist = F.pairwise_distance(feat1,feat2)\n",
    "\n",
    "    loss = criterion(feat1, feat2, trainLabel)\n",
    "\n",
    "    print(f\"dist is {float(dist)} and loss is {loss}\")\n",
    "\n",
    "\n",
    "\n",
    "def getDistribution(support, labels ):\n",
    "\n",
    "    uniqueClasses = sorted(pd.Series(labels).unique())\n",
    "\n",
    "    NUM_COLORS = len(uniqueClasses)\n",
    "\n",
    "    \n",
    "    # get_color = matplotlib.colors.LinearSegmentedColormap.from_list(mp, colors=['r', 'y', 'g', 'b'], N=NUM_COLORS)\n",
    "\n",
    "    get_color = plt.get_cmap('viridis')\n",
    "\n",
    "    colorMap = {}\n",
    "    for i in range(len(uniqueClasses)):\n",
    "        colorMap[uniqueClasses[i]] = get_color(i/NUM_COLORS)\n",
    "    \n",
    "    support = support.cpu().numpy()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(support)\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    pcaX = pca.fit_transform(X=X)\n",
    "    varExplained = np.sum(pca.explained_variance_ratio_[:2])\n",
    "    print(f\"Variance Explained : {varExplained}\")\n",
    "\n",
    "    colors = [colorMap[int(x)] for x in labels]\n",
    "    for i in range(len(uniqueClasses)):\n",
    "      index = np.array(labels) == int(uniqueClasses[i])\n",
    "      plt.scatter(pcaX[index,0], pcaX[index,1], color=colorMap[int(uniqueClasses[i])], label =int(uniqueClasses[i]))\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def getClassMap(uniqueClass):\n",
    "\n",
    "    classMap = {}\n",
    "    for i in range(len(uniqueClass)):\n",
    "        classMap[uniqueClass[i]] = i\n",
    "\n",
    "    classMap_ = {k:v for v,k in classMap.items()}\n",
    "\n",
    "    return classMap, classMap_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPred(net, train_dataloader_, testDataset_, metric = 'cosine', usePCA = False, PCA_components = 10):\n",
    "\n",
    "    support, labels = getSupport(net, train_dataloader_)\n",
    "\n",
    "    X = support.cpu().numpy()\n",
    "\n",
    "    pca = PCA(n_components=PCA_components)\n",
    "    if usePCA:\n",
    "      X = pca.fit_transform(X=X)\n",
    "\n",
    "    knnModel = KNeighborsClassifier(n_neighbors=8,metric=metric)\n",
    "    knnModel.fit(X, labels)\n",
    "\n",
    "    labelList = []\n",
    "    predList = []\n",
    "    for i in tqdm(range(len(testDataset_))):\n",
    "        testX = net.getFeatures(testDataset_[i][0].unsqueeze(0).cuda()).detach().cpu().numpy()\n",
    "        label = int(testDataset_[i][1])\n",
    "        if usePCA:\n",
    "          testX = pca.transform(testX)\n",
    "\n",
    "        pred = knnModel.predict(testX)\n",
    "        predList.append(pred[0])\n",
    "        labelList.append(label)\n",
    "\n",
    "    \n",
    "    return predList, labelList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareDist(net, criterion, img1, img2, label,  metric = 'cosine',printOutput=True):\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    output1 = net.getFeatures(img1)\n",
    "    output2 = net.getFeatures(img2)\n",
    "\n",
    "    if metric == 'cosine':\n",
    "        dist = 1 - torch.nn.functional.cosine_similarity(output1, output2)\n",
    "    else:\n",
    "        dist = torch.nn.functional.pairwise_distance(output1, output2)\n",
    "        \n",
    "    loss =  criterion(output1, output2, label) \n",
    "\n",
    "    dist = dist.detach().cpu().numpy()\n",
    "\n",
    "    dist = np.round(dist[0],3)\n",
    "\n",
    "    if printOutput:\n",
    "        print(f\"Dist : {dist} Loss : {loss} Target Label : {int(label)}\")\n",
    "\n",
    "    return dist, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(net, imgList, labelList, testDataset_):\n",
    "\n",
    "    correct = 0\n",
    "    net.eval()\n",
    "    predLabel_list = []\n",
    "    pred_list = []\n",
    "    trainFeat = getFeat(net,imgList)\n",
    "#1\n",
    "#     from sklearn.svm import SVC\n",
    "#     knnModel = SVC(gamma='auto', kernel='rbf')\n",
    "\n",
    "#2     from sklearn.ensemble import RandomForestClassifier\n",
    "#     knnModel = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "#     from sklearn.ensemble import GradientBoostingClassifier\n",
    "#     knnModel = GradientBoostingClassifier()\n",
    "    \n",
    "#4\n",
    "#     from sklearn.naive_bayes import GaussianNB\n",
    " \n",
    "#     knnModel = GaussianNB()\n",
    "#5\n",
    "    knnModel = KNeighborsClassifier(n_neighbors=10, metric='cosine')\n",
    "    knnModel.fit(trainFeat, labelList)\n",
    "\n",
    "    for i in tqdm(range(len(testDataset_))):\n",
    "        predImg, predLabel = testDataset_[i]\n",
    "        predImg = predImg.cuda().unsqueeze(0)\n",
    "        predLabel_list.append(predLabel)\n",
    "    \n",
    "        testFeat = getFeat(net, [predImg])\n",
    "        pred = knnModel.predict(testFeat)\n",
    "        pred_list.extend(pred)\n",
    "#         print(predLabel)\n",
    "#         print(pred)\n",
    "        if pred == predLabel:\n",
    "            correct += 1\n",
    "#     print(predLabel_list)\n",
    "#     print(pred_list)\n",
    "    class_report=classification_report( predLabel_list, pred_list, target_names=['class 0', 'class 1'])\n",
    "    # print(f\"Accuracy is : {correct/len(testDataset_)}\")\n",
    "    print(class_report)\n",
    "    acc = np.round(correct/len(testDataset_),4)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel_(net, imgList, labelList):\n",
    "\n",
    "    correct = 0\n",
    "    net.eval()\n",
    "\n",
    "    trainFeat = getFeat(net,imgList)\n",
    "    from sklearn.ensemble import  AdaBoostClassifier \n",
    " \n",
    "    knnModel = AdaBoostClassifier()#0.854\n",
    "#     from sklearn import tree\n",
    "#     knnModel = tree.DecisionTreeClassifier()# 0.83\n",
    "#     knnModel = KNeighborsClassifier(n_neighbors=10, metric='cosine')#0.90\n",
    "    knnModel.fit(trainFeat, labelList)\n",
    "\n",
    "    for i in tqdm(range(len(imgList))):\n",
    "        pred = knnModel.predict(trainFeat[i,:].reshape(1,-1))\n",
    "\n",
    "        if pred == labelList[i]:\n",
    "            correct += 1\n",
    "\n",
    "    # print(f\"Accuracy is : {correct/len(imgList)}\")\n",
    "\n",
    "    acc = np.round(correct/len(imgList),4)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictModel(net, test_dataloader_):\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    labelList = []\n",
    "    predList = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for (img,labels) in test_dataloader_:\n",
    "            img,labels = img.cuda(), labels.cuda()\n",
    "            \n",
    "            outputs = net(img)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            labelList = labelList + list(labels.cpu().numpy())\n",
    "            predList = predList + list(preds.cpu().numpy())\n",
    "\n",
    "    return labelList, predList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, df, prob = 0.5, transform = None, classMap= None, classMap_ = None):\n",
    "#     \"\"\" \n",
    "#     df : Dataframe with filename and label columns.\n",
    "\n",
    "\n",
    "#     \"\"\"\n",
    "    \n",
    "    self.df = df\n",
    "    self.transform = transform\n",
    "    self.numClasses = df.label.nunique()\n",
    "    self.uniqueLabels = sorted(df.label.unique())\n",
    "    self.prob = prob\n",
    "\n",
    "    if classMap is None or classMap_ is None:\n",
    "        self.classMap, self.classMap_ = getClassMap(self.df.label.unique())\n",
    "    else:\n",
    "        self.classMap, self.classMap_ = classMap, classMap_\n",
    "        \n",
    "  def __len__(self):\n",
    "    \n",
    "    return len(self.df)\n",
    "    \n",
    "  def __getitem__(self, index):\n",
    "\n",
    "    is_positive = np.random.random() < self.prob\n",
    "\n",
    "    if is_positive:\n",
    "        posLabel = np.random.choice(self.uniqueLabels,1)[0]\n",
    "\n",
    "        # print(posLabel)\n",
    "        subDf = self.df[self.df.label == posLabel] \n",
    "        subDf = subDf.sample(2, replace = False)\n",
    "            \n",
    "        filename1, filename2 = subDf.iloc[0]['filepath'], subDf.iloc[1]['filepath']\n",
    "        img1, img2 =  Image.open(os.path.join(filename1)).convert('RGB'), Image.open(os.path.join(filename2)).convert('RGB')\n",
    "        \n",
    "        target_label = 1\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "            \n",
    "        # img1 = img1/255.0\n",
    "        # img2 = img2/255.0\n",
    "\n",
    "    else:\n",
    "\n",
    "        negLabels = np.random.choice(self.uniqueLabels,2, replace = False)\n",
    "\n",
    "\n",
    "        subDf1 = self.df[self.df.label == negLabels[0]].sample(1)\n",
    "        subDf2 = self.df[self.df.label == negLabels[1]].sample(1)\n",
    "\n",
    "        filename1, filename2 = subDf1.iloc[0]['filepath'], subDf2.iloc[0]['filepath']\n",
    "        img1, img2 =  Image.open(os.path.join(filename1)).convert('RGB'), Image.open(os.path.join(filename2)).convert('RGB')\n",
    "\n",
    "        target_label = 0\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "            \n",
    "        # img1 = img1/255.0\n",
    "        # img2 = img2/255.0\n",
    "\n",
    "\n",
    "    \n",
    "    return img1,img2, target_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, df, transform, classMap= None, classMap_ = None):\n",
    "    \n",
    "    \n",
    "    self.df = df\n",
    "    self.transform = transform\n",
    "\n",
    "    if classMap is None or classMap_ is None:\n",
    "        self.classMap, self.classMap_ = getClassMap(self.df.label.unique())\n",
    "    else:\n",
    "        self.classMap, self.classMap_ = classMap, classMap_\n",
    "        \n",
    "    \n",
    "  def __len__(self):\n",
    "    \n",
    "    return len(self.df)\n",
    "    \n",
    "  def __getitem__(self, index):\n",
    "    \n",
    "    filename = self.df.iloc[index]['filepath']\n",
    "    \n",
    "    img =  Image.open(os.path.join(filename)).convert('RGB')\n",
    "\n",
    "    \n",
    "    label = self.classMap[self.df.iloc[index]['label']]\n",
    "    \n",
    "    if self.transform is not None:\n",
    "        img = self.transform(img)\n",
    "    \n",
    "    \n",
    "    # img = img/255.0\n",
    "    \n",
    "    return img, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleHook():\n",
    "    def __init__(self, module, backward=True):\n",
    "\n",
    "        self.gradList = []\n",
    "\n",
    "        if backward==False:\n",
    "            self.hook = module.register_forward_hook(self.hook_fn)\n",
    "        else:\n",
    "            self.hook = module.register_backward_hook(self.hook_fn)\n",
    "    \n",
    "    def __getitem__(self,key):\n",
    "        return self.gradList[key]\n",
    "\n",
    "    def hook_fn(self, module, grad_input, grad_output):\n",
    "        self.gradList.append(grad_output.cpu().numpy())\n",
    "\n",
    "    def close(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "\n",
    "class ParamHook():\n",
    "    def __init__(self, param):\n",
    "        self.gradList = []\n",
    "        self.hook = param.register_hook(self.hook_fn)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.gradList[key]\n",
    "      \n",
    "    def hook_fn(self, grad):\n",
    "        meanGrad = grad.abs().mean().cpu().numpy()\n",
    "        maxGrad = grad.abs().max().cpu().numpy()\n",
    "        self.gradList.append([meanGrad,maxGrad])\n",
    "\n",
    "    def close(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "\n",
    "class gradCapture():\n",
    "\n",
    "   def __init__(self):\n",
    "     self.paramGradList = {}\n",
    "     self.gradList = {}\n",
    "\n",
    "     self.paramHooks = {}\n",
    "     self.moduleHooks = {}\n",
    "\n",
    "\n",
    "   def registerHooks(self, model):\n",
    "\n",
    "     for name, param in net.named_parameters():\n",
    "       self.paramHooks[name] = ParamHook(param)\n",
    "\n",
    "   def plotGrad(self, paramList, plotMean = True, plotRange = None):\n",
    "\n",
    "     length = len(self.paramHooks[list(self.paramHooks.keys())[0]].gradList)\n",
    "     \n",
    "     if plotRange :\n",
    "         x = list(range(plotRange[0], plotRange[1]))\n",
    "     else:\n",
    "         x = list(range(length))\n",
    "\n",
    "     for i in range(len(paramList)):\n",
    "       key = paramList[i]\n",
    "       if plotMean:\n",
    "         y = [i[0] for i in gradCap.paramHooks[key].gradList]\n",
    "       else:\n",
    "         y = [i[1] for i in gradCap.paramHooks[key].gradList]\n",
    "\n",
    "       if plotRange :\n",
    "        y = np.array(y)[plotRange[0]:plotRange[1]]\n",
    "    \n",
    "        \n",
    "       plt.plot(x,y,label=key)\n",
    "\n",
    "     plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=15.0, metric='cosine'):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.metric = metric\n",
    "\n",
    "    def forward_cosine(self, output1, output2, label):\n",
    "        euclidean_distance = 1 - torch.nn.functional.cosine_similarity(output1, output2)\n",
    "        # print(euclidean_distance)\n",
    "        loss_contrastive = torch.mean((label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (1 - label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        \n",
    "        return loss_contrastive\n",
    "      \n",
    "    def forward_euclidean(self, output1, output2, label):\n",
    "        euclidean_distance = torch.nn.functional.pairwise_distance(output1, output2)\n",
    "#         euclidean_distance =euclidean_distance.to(output1.device)\n",
    "        label = label.float()\n",
    "        euclidean_distance =euclidean_distance.float()\n",
    "        a = (label) * torch.pow(euclidean_distance, 2)\n",
    "        b = (1 - label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        loss_contrastive =torch.mean( a + b)\n",
    "        return loss_contrastive\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "\n",
    "      if self.metric =='cosine':\n",
    "        loss = self.forward_cosine(output1, output2, label)\n",
    "      elif self.metric=='euclidean':\n",
    "        loss = self.forward_euclidean(output1, output2, label)\n",
    "      \n",
    "      return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "                                      transforms.Resize((224,224)),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "                                     transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN_SAMPLES = 310\n",
    "NUM_TEST_SAMPLES = 110\n",
    "STEPS_PER_EPOCH = 250\n",
    "NUM_EPOCHS =50\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes=512, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1   = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2   = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca =ChannelAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=3):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = SpatialAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg_feature = torchvision.models.vgg16(pretrained=True).features\n",
    "vgg_16 = models.vgg16(pretrained=False)\n",
    "pthfile = r'vgg16-397923af.pth'\n",
    "vgg_16.load_state_dict(torch.load(pthfile))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in vgg_16.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name)\n",
    "#         print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_feature = vgg_16.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU(inplace)\n",
      "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU(inplace)\n",
      "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (6): ReLU(inplace)\n",
      "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): ReLU(inplace)\n",
      "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(inplace)\n",
      "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (13): ReLU(inplace)\n",
      "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): ReLU(inplace)\n",
      "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (18): ReLU(inplace)\n",
      "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (20): ReLU(inplace)\n",
      "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (22): ReLU(inplace)\n",
      "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (25): ReLU(inplace)\n",
      "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (27): ReLU(inplace)\n",
      "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (29): ReLU(inplace)\n",
      "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(vgg_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vgg16_Siamese(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(vgg16_Siamese, self).__init__()\n",
    "        \n",
    "        self.vgg_features = vgg_feature\n",
    "        self.activation = nn.ReLU()\n",
    "        self.ca = ca\n",
    "        self.sa =sa\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(25088, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, 1000),\n",
    "            \n",
    "        )\n",
    "\n",
    "        \n",
    "    def getFeatures(self,x):\n",
    "\n",
    "        x = self.vgg_features(x)\n",
    "#         x = self.ca(x)*x\n",
    "        x = self.sa(x)*x\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self,x1, x2):\n",
    "\n",
    "        x1 = self.getFeatures(x1)\n",
    "        x2 = self.getFeatures(x2)\n",
    "\n",
    "        return x1, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=vgg16_Siamese()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vgg16_Siamese(\n",
       "  (vgg_features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (activation): ReLU()\n",
       "  (ca): ChannelAttention(\n",
       "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "    (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (relu1): ReLU()\n",
       "    (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       "  (sa): SpatialAttention(\n",
       "    (conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    net = nn.DataParallel(net)\n",
    "        # optimizer = nn.DataParallel(optimizer)\n",
    "\n",
    "net.to(device) #加入此句，可以解决相应问题\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "gradCap = gradCapture()\n",
    "gradCap.registerHooks(net)\n",
    "\n",
    "criterion = ContrastiveLoss(margin=0.4, metric='euclidean')\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()) ,lr = 0.00001 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf,testDf = load_data(train_size = NUM_TRAIN_SAMPLES, test_size = NUM_TEST_SAMPLES)\n",
    "\n",
    "trainDataset = SiameseDataset(trainDf, prob = 0.5, transform=train_transform)\n",
    "\n",
    "trainDataset_ = ImageDataset(trainDf, transform = train_transform)\n",
    "testDataset_ = ImageDataset(testDf, transform = test_transform, classMap = trainDataset_.classMap, classMap_= trainDataset_.classMap_)\n",
    "\n",
    "train_dataloader = DataLoader(trainDataset,\n",
    "                        shuffle=True,\n",
    "                         num_workers=0,\n",
    "                        batch_size=50)\n",
    "\n",
    "train_dataloader_ = DataLoader(trainDataset_,\n",
    "                        shuffle=False,\n",
    "                         num_workers=0,\n",
    "                        batch_size=50)\n",
    "\n",
    "test_dataloader_ = DataLoader(testDataset_,\n",
    "                        shuffle=False,\n",
    "#                         num_workers=1,\n",
    "                        batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = list(range(NUM_TRAIN_SAMPLES))\n",
    "\n",
    "trainImgList = []\n",
    "trainLabelList = []\n",
    "\n",
    "for i in range(len(indexes)):\n",
    "    x, y = trainDataset_[indexes[i]]\n",
    "    trainImgList.append(x)\n",
    "    trainLabelList.append(y)\n",
    "    trainImgList[i] = trainImgList[i].cuda().unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13.9044, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(12.7323, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(7.1538, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(5.5842, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(4.3038, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(2.6150, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(2.9876, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 7.0401527881622314\n",
      "tensor(1.5524, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.9972, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(1.0319, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.7726, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.5433, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.5586, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.4123, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.8383189056600843\n",
      "tensor(0.4407, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.3841, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.3012, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.3384, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.2248, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.2680, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.3166, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.3248299743447985\n",
      "tensor(0.2071, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.1797, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.1639, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.1147, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.1206, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0904, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.1061, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.14036879049880163\n",
      "tensor(0.1077, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.1165, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0726, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0966, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0587, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0802, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.1065, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.09126132407358714\n",
      "tensor(0.0644, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0679, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0499, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0641, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0579, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0501, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0318, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.05517024866172245\n",
      "tensor(0.0483, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0455, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0434, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0539, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0529, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0440, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0392, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.04674969241023064\n",
      "tensor(0.0462, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0485, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0429, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0406, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0403, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0420, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0379, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.04264486261776516\n",
      "tensor(0.0381, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0424, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0425, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0406, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0384, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0369, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0488, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.04108439385890961\n",
      "tensor(0.0402, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0396, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0394, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0390, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0420, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0409, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0404, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.04021276054637773\n",
      "tensor(0.0411, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0428, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0413, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0404, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0412, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0411, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0440, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.04171805190188544\n",
      "tensor(0.0388, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0417, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0407, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0409, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0387, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0445, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0369, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.04033101562942777\n",
      "tensor(0.0396, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0422, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0381, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0409, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0400, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0415, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0362, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.039782314960445674\n",
      "tensor(0.0403, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0391, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0394, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0429, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0422, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0407, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0470, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.041632694857461114\n",
      "tensor(0.0393, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0411, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0417, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0396, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0427, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0441, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0435, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.04171355973396983\n",
      "tensor(0.0434, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0402, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0390, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0408, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0411, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0400, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0372, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.04024842807224819\n",
      "tensor(0.0408, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0421, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0426, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0387, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0386, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0384, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0470, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.04118289000221661\n",
      "tensor(0.0403, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0414, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0380, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0387, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0423, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0412, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0361, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.039706818759441376\n",
      "tensor(0.0386, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0402, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0380, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0406, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0418, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0420, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0426, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.04054321348667145\n",
      "tensor(0.0398, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0403, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0404, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0419, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0427, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0390, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0426, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.040972870375428884\n",
      "tensor(0.0410, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0409, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0410, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0397, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0418, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0396, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0414, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.040777714656932015\n",
      "tensor(0.0417, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0402, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0403, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0398, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0430, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0373, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0370, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.0399208308330604\n",
      "tensor(0.0394, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0411, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0425, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0418, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0368, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0419, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0414, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.04069516382047108\n",
      "tensor(0.0411, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0405, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0408, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0377, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0420, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0418, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0475, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.041623856872320175\n",
      "tensor(0.0375, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0435, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0411, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0424, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0406, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0404, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0420, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.041051161608525684\n",
      "tensor(0.0419, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0390, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0413, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0410, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0395, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0399, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0374, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.03999570916805949\n",
      "tensor(0.0394, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0397, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0413, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0389, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0412, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0413, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0385, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.04003848348345075\n",
      "tensor(0.0401, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0413, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0415, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0409, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0419, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0399, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0392, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.0406927689909935\n",
      "tensor(0.0415, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0402, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0395, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0401, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0413, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0400, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0467, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.04132113818611417\n",
      "tensor(0.0416, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0427, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0379, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0402, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0416, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0406, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0396, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.04061290568539074\n",
      "tensor(0.0399, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0413, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0404, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0413, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0407, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0393, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0365, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.039909422397613525\n",
      "tensor(0.0426, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0398, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0425, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0386, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0406, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0398, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0398, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.04054252164704459\n",
      "tensor(0.0388, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0401, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0394, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0384, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0415, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0407, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0420, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.04012986219355038\n",
      "tensor(0.0400, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0386, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0405, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0412, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0413, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0386, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0366, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.039558244603020806\n",
      "tensor(0.0409, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0416, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0408, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0421, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0422, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0411, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0431, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.04168706227626119\n",
      "tensor(0.0388, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0406, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0410, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0386, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0421, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0400, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0403, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.04019993277532714\n",
      "tensor(0.0402, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0399, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0408, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0393, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0417, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0394, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0398, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.04015700252992766\n",
      "tensor(0.0409, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0387, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0388, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0401, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0408, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0410, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0369, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.03959838513817106\n",
      "tensor(0.0417, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0392, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0402, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0403, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0432, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0386, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0414, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.04066501344953265\n",
      "tensor(0.0434, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0394, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0426, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0383, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0403, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0392, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0396, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.04039543494582176\n",
      "tensor(0.0403, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0410, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0396, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0410, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0400, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0421, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0428, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.04097416251897812\n",
      "tensor(0.0411, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0391, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0395, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0407, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0397, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0385, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0373, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.0394365984414305\n",
      "tensor(0.0396, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0410, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0404, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0414, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0395, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0413, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0411, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.04059156881911414\n",
      "tensor(0.0410, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0397, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0415, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0421, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0400, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0408, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0372, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.04031206827078547\n",
      "tensor(0.0396, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0403, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0401, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0413, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0407, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0383, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0384, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.03982352146080562\n",
      "tensor(0.0385, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0410, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0393, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0381, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0439, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0382, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0461, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.040756434202194214\n",
      "tensor(0.0425, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0413, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0405, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0393, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0419, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0429, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0417, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.04145178305251258\n",
      "tensor(0.0397, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0406, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0390, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0400, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0390, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0402, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0391, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.039653580103601725\n",
      "tensor(0.0396, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0391, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0386, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0400, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0419, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0413, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0414, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.04028042459062168\n",
      "tensor(0.0401, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0433, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0399, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0396, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0434, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0409, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "tensor(0.0380, device='cuda:0', grad_fn=<MeanBackward1>)\n",
      "Epoch Loss : 0.040744048676320484\n"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = []\n",
    "#     support, labels = getSupport(net, train_dataloader_)\n",
    "#     getDistribution(support, labels)\n",
    "#     plt.show()\n",
    "\n",
    "    for i, data in enumerate(train_dataloader,0):\n",
    "        img0, img1 , label = data\n",
    "        img0, img1 , label = img0.cuda(), img1.cuda() , label.cuda()\n",
    " \n",
    "        optimizer.zero_grad()\n",
    "        output1,output2 = net(img0,img1)\n",
    "        loss_contrastive = criterion(output1,output2,label)\n",
    "        print(loss_contrastive)\n",
    "        loss_contrastive.mean().backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss.append(loss_contrastive.item())\n",
    "\n",
    "\n",
    "    print(f\"Epoch Loss : {np.mean(epoch_loss)}\")\n",
    "\n",
    "\n",
    "# support, labels = getSupport(net, train_dataloader_)\n",
    "# getDistribution(support, labels)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(net,torch.nn.DataParallel):\n",
    "\t\tnet = net.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:18<00:00,  6.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.51      0.96      0.67        49\n",
      "     class 1       0.89      0.26      0.41        61\n",
      "\n",
      "   micro avg       0.57      0.57      0.57       110\n",
      "   macro avg       0.70      0.61      0.54       110\n",
      "weighted avg       0.72      0.57      0.52       110\n",
      "\n",
      " Test Acc : 0.5727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "testAcc = testModel(net,trainImgList, trainLabelList, testDataset_)\n",
    "print(f\" Test Acc : {testAcc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test Acc : 0.6818\n"
     ]
    }
   ],
   "source": [
    "print(f\" Test Acc : {testAcc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "无\n",
    "\n",
    "第一次\n",
    "weighted avg       0.91      0.90      0.89       110\n",
    "\n",
    "Test Acc : 0.90\n",
    "\n",
    "第二次\n",
    "weighted avg       0.86      0.85      0.84       110\n",
    "\n",
    "Test Acc : 0.85\n",
    "\n",
    "第三次\n",
    "weighted avg       0.93      0.92      0.91       110\n",
    "\n",
    "Test Acc : 0.92\n",
    "\n",
    "第四次\n",
    "weighted avg       0.91      0.90      0.89       110\n",
    "\n",
    "Test Acc : 0.90\n",
    "\n",
    "第五次\n",
    "weighted avg       0.94      0.93     0.92       110\n",
    "\n",
    "Test Acc : 0.93\n",
    "\n",
    "\n",
    "\n",
    "只有channel_attention\n",
    "\n",
    "第一次 \n",
    "\n",
    "weighted avg       0.71      0.70      0.70       110\n",
    "\n",
    "Test Acc : 0.7\n",
    "\n",
    "第二次\n",
    "\n",
    "weighted avg       0.64      0.64      0.63       110\n",
    "\n",
    " Test Acc : 0.6364\n",
    " \n",
    "第三次\n",
    "weighted avg       0.71      0.70      0.70       110\n",
    "\n",
    "Test Acc : 0.7\n",
    "\n",
    "第四次\n",
    "\n",
    "weighted avg       0.69      0.69      0.69       110\n",
    "\n",
    " Test Acc : 0.6909\n",
    "\n",
    "第五次\n",
    "\n",
    "weighted avg       0.66      0.64      0.64       110\n",
    "\n",
    " Test Acc : 0.6364"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只有 spatial_attention\n",
    "\n",
    "1.\n",
    "\n",
    "weighted avg       0.65      0.64      0.63       110\n",
    "\n",
    " Test Acc : 0.6364\n",
    " \n",
    "2.\n",
    "\n",
    "weighted avg       0.66      0.65      0.65       110\n",
    "\n",
    " Test Acc : 0.6545\n",
    " \n",
    "3.\n",
    "\n",
    "weighted avg       0.70      0.68      0.68       110\n",
    "\n",
    " Test Acc : 0.6818\n",
    " \n",
    "4.\n",
    "\n",
    "weighted avg       0.71      0.67      0.67       110\n",
    "\n",
    " Test Acc : 0.6727\n",
    " \n",
    "5.\n",
    "\n",
    "weighted avg       0.72      0.57      0.52       110\n",
    "\n",
    " Test Acc : 0.5727"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
